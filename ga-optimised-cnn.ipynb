{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import copy\nimport torch\nimport random\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms as T\n\nfrom tqdm import tqdm\nfrom typing import List, Tuple\nfrom dataclasses import dataclass\nfrom datasets import load_dataset\nfrom torch.utils.data import DataLoader, Dataset\n\ntorch.manual_seed(42)\nnp.random.seed(42)\nrandom.seed(42)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:33:34.678003Z","iopub.execute_input":"2025-09-15T06:33:34.678800Z","iopub.status.idle":"2025-09-15T06:33:34.684569Z","shell.execute_reply.started":"2025-09-15T06:33:34.678773Z","shell.execute_reply":"2025-09-15T06:33:34.683735Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"@dataclass\nclass Hyperparameters:\n    width_mult: float\n    learning_rate: float\n    batch_size: int\n    dropout_rate: float\n    weight_decay: float\n    momentum: float\n    conv_channels: List[int]\n    \n    def __post_init__(self):\n        self.width_mult = max(0.25, min(2.0, self.width_mult))\n        self.learning_rate = max(0.0001, min(0.1, self.learning_rate))\n        self.batch_size = max(32, min(256, int(self.batch_size)))\n        self.dropout_rate = max(0.0, min(0.5, self.dropout_rate))\n        self.weight_decay = max(1e-6, min(1e-2, self.weight_decay))\n        self.momentum = max(0.1, min(0.99, self.momentum))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:33:34.685713Z","iopub.execute_input":"2025-09-15T06:33:34.685938Z","iopub.status.idle":"2025-09-15T06:33:34.704544Z","shell.execute_reply.started":"2025-09-15T06:33:34.685913Z","shell.execute_reply":"2025-09-15T06:33:34.703812Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"class FEMNISTDataset(Dataset):\n    def __init__(self, hf_split, transform):\n        self.data = hf_split\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        item = self.data[idx]\n        img = item['image']\n        label = int(item['character'])\n        if self.transform:\n            img = self.transform(img)\n        return img, label","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:33:34.705185Z","iopub.execute_input":"2025-09-15T06:33:34.705423Z","iopub.status.idle":"2025-09-15T06:33:34.719160Z","shell.execute_reply.started":"2025-09-15T06:33:34.705383Z","shell.execute_reply":"2025-09-15T06:33:34.718645Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"class SeparableConv(nn.Module):\n    def __init__(self, in_ch, out_ch, kernel=3, stride=1, padding=1, dropout_rate=0.0):\n        super().__init__()\n        self.dw = nn.Conv2d(in_ch, in_ch, kernel, stride, padding, groups=in_ch, bias=False)\n        self.pw = nn.Conv2d(in_ch, out_ch, 1, 1, 0, bias=False)\n        self.bn = nn.BatchNorm2d(out_ch)\n        self.dropout = nn.Dropout2d(dropout_rate) if dropout_rate > 0 else nn.Identity()\n        \n    def forward(self, x):\n        x = self.dw(x)\n        x = self.pw(x)\n        x = self.bn(x)\n        x = F.relu(x, inplace=True)\n        return self.dropout(x)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:33:34.720410Z","iopub.execute_input":"2025-09-15T06:33:34.720619Z","iopub.status.idle":"2025-09-15T06:33:34.736074Z","shell.execute_reply.started":"2025-09-15T06:33:34.720605Z","shell.execute_reply":"2025-09-15T06:33:34.735365Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"class CNN(nn.Module):\n    def __init__(self, num_classes=62, hyperparams=None):\n        super().__init__()\n        if hyperparams is None:\n            hyperparams = Hyperparameters(\n                width_mult=1.0, learning_rate=0.01, batch_size=128,\n                dropout_rate=0.2, weight_decay=2e-4, momentum=0.9,\n                conv_channels=[16, 32, 64, 128]\n            )\n        \n        self.hyperparams = hyperparams\n        def c(ch): return max(8, int(ch * hyperparams.width_mult))\n        \n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, c(hyperparams.conv_channels[0]), 3, 1, 1, bias=False),\n            nn.BatchNorm2d(c(hyperparams.conv_channels[0])),\n            nn.ReLU(inplace=True),\n            nn.Dropout2d(hyperparams.dropout_rate * 0.5)\n        )\n        \n        self.sep1 = SeparableConv(\n            c(hyperparams.conv_channels[0]), c(hyperparams.conv_channels[1]), \n            stride=2, dropout_rate=hyperparams.dropout_rate * 0.7\n        )\n        self.sep2 = SeparableConv(\n            c(hyperparams.conv_channels[1]), c(hyperparams.conv_channels[2]), \n            stride=2, dropout_rate=hyperparams.dropout_rate * 0.8\n        )\n        self.sep3 = SeparableConv(\n            c(hyperparams.conv_channels[2]), c(hyperparams.conv_channels[3]), \n            stride=2, dropout_rate=hyperparams.dropout_rate\n        )\n        \n        self.pool = nn.AdaptiveAvgPool2d(1)\n        self.dropout_fc = nn.Dropout(hyperparams.dropout_rate)\n        self.fc = nn.Linear(c(hyperparams.conv_channels[3]), num_classes)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.sep1(x)\n        x = self.sep2(x)\n        x = self.sep3(x)\n        x = self.pool(x)\n        x = self.dropout_fc(x.view(x.size(0), -1))\n        return self.fc(x)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:33:34.736757Z","iopub.execute_input":"2025-09-15T06:33:34.736980Z","iopub.status.idle":"2025-09-15T06:33:34.750600Z","shell.execute_reply.started":"2025-09-15T06:33:34.736964Z","shell.execute_reply":"2025-09-15T06:33:34.750060Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class GeneticAlgorithm:\n    def __init__(self, population_size=8, mutation_rate=0.3, crossover_rate=0.7):\n        self.population_size = population_size\n        self.mutation_rate = mutation_rate\n        self.crossover_rate = crossover_rate\n        self.generation = 0\n        \n    def create_individual(self):\n        return Hyperparameters(\n            width_mult=random.uniform(0.5, 1.5),\n            learning_rate=random.uniform(0.005, 0.05),\n            batch_size=random.choice([64, 96, 128, 160, 192]),\n            dropout_rate=random.uniform(0.1, 0.4),\n            weight_decay=random.uniform(1e-5, 5e-3),\n            momentum=random.uniform(0.8, 0.95),\n            conv_channels=[16, 32, 64, 128]\n        )\n    \n    def create_population(self):\n        population = []\n        baseline = Hyperparameters(\n            width_mult=1.0, learning_rate=0.01, batch_size=128,\n            dropout_rate=0.2, weight_decay=2e-4, momentum=0.9,\n            conv_channels=[16, 32, 64, 128]\n        )\n        population.append(baseline)\n        \n        for _ in range(self.population_size - 1):\n            population.append(self.create_individual())\n        return population\n    \n    def crossover(self, parent1, parent2) :\n        child1_dict = {}\n        child2_dict = {}\n        \n        for key in parent1.__dict__.keys():\n            if random.random() < 0.5:\n                child1_dict[key] = getattr(parent1, key)\n                child2_dict[key] = getattr(parent2, key)\n            else:\n                child1_dict[key] = getattr(parent2, key)\n                child2_dict[key] = getattr(parent1, key)\n        \n        child1 = Hyperparameters(**child1_dict)\n        child2 = Hyperparameters(**child2_dict)\n        return child1, child2\n    \n    def mutate(self, individual):\n        mutated = copy.deepcopy(individual)\n        \n        if random.random() < self.mutation_rate:\n            mutated.width_mult += random.gauss(0, 0.2)\n        if random.random() < self.mutation_rate:\n            mutated.learning_rate *= random.uniform(0.5, 2.0)\n        if random.random() < self.mutation_rate:\n            mutated.batch_size = random.choice([64, 96, 128, 160, 192])\n        if random.random() < self.mutation_rate:\n            mutated.dropout_rate += random.gauss(0, 0.1)\n        if random.random() < self.mutation_rate:\n            mutated.weight_decay *= random.uniform(0.1, 10.0)\n        if random.random() < self.mutation_rate:\n            mutated.momentum += random.gauss(0, 0.05)\n            \n        mutated.__post_init__()\n        return mutated\n    \n    def select_parents(self, population, fitness_scores):\n        selected = []\n        tournament_size = 3\n        \n        for _ in range(len(population)):\n            tournament_indices = random.sample(range(len(population)), min(tournament_size, len(population)))\n            tournament_fitness = [fitness_scores[i] for i in tournament_indices]\n            winner_idx = tournament_indices[np.argmax(tournament_fitness)]\n            selected.append(copy.deepcopy(population[winner_idx]))\n        \n        return selected","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:33:34.751266Z","iopub.execute_input":"2025-09-15T06:33:34.751486Z","iopub.status.idle":"2025-09-15T06:33:34.771266Z","shell.execute_reply.started":"2025-09-15T06:33:34.751465Z","shell.execute_reply":"2025-09-15T06:33:34.770696Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def train_and_evaluate_model(hyperparams, train_loader, val_loader, device, max_epochs=3):\n    try:\n        model = CNN(num_classes=62, hyperparams=hyperparams).to(device)\n        \n        optimizer = torch.optim.SGD(\n            model.parameters(), \n            lr=hyperparams.learning_rate,\n            momentum=hyperparams.momentum,\n            weight_decay=hyperparams.weight_decay\n        )\n        criterion = nn.CrossEntropyLoss()\n        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n        \n        best_val_acc = 0.0\n        \n        for epoch in range(max_epochs):\n            model.train()\n            for batch_idx, (data, target) in enumerate(train_loader):\n                if batch_idx > 200:  # Limit training for GA speed\n                    break\n                    \n                data, target = data.to(device), target.to(device)\n                optimizer.zero_grad()\n                output = model(data)\n                loss = criterion(output, target)\n                loss.backward()\n                optimizer.step()\n            \n            model.eval()\n            correct = 0\n            total = 0\n            with torch.no_grad():\n                for batch_idx, (data, target) in enumerate(val_loader):\n                    if batch_idx > 50:\n                        break\n                    data, target = data.to(device), target.to(device)\n                    output = model(data)\n                    pred = output.argmax(dim=1)\n                    correct += pred.eq(target).sum().item()\n                    total += target.size(0)\n            \n            val_acc = correct / total\n            best_val_acc = max(best_val_acc, val_acc)\n            scheduler.step()\n        \n        return best_val_acc\n    \n    except Exception as e:\n        print(f\"Error training model: {e}\")\n        return 0.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:33:34.860663Z","iopub.execute_input":"2025-09-15T06:33:34.860842Z","iopub.status.idle":"2025-09-15T06:33:34.867886Z","shell.execute_reply.started":"2025-09-15T06:33:34.860827Z","shell.execute_reply":"2025-09-15T06:33:34.867130Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"print(\"Loading FEMNIST dataset...\")\nds = load_dataset(\"flwrlabs/femnist\")\n\ntransform = T.Compose([\n    T.Resize((28,28)),\n    T.Grayscale(num_output_channels=1),\n    T.ToTensor(),\n    T.Normalize((0.5,), (0.5,))\n])\n\ntemp_split = ds['train'].train_test_split(test_size=0.1, seed=42)\ntest_split = temp_split['test']\n\ntrain_valid = temp_split['train'].train_test_split(test_size=0.1, seed=42)\n\nga_train_size = 20000\nga_val_size = 5000\n\nga_train_data = train_valid['train'].select(range(min(ga_train_size, len(train_valid['train']))))\nga_val_data = train_valid['test'].select(range(min(ga_val_size, len(train_valid['test']))))\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:33:34.869305Z","iopub.execute_input":"2025-09-15T06:33:34.869606Z","iopub.status.idle":"2025-09-15T06:33:43.306475Z","shell.execute_reply.started":"2025-09-15T06:33:34.869588Z","shell.execute_reply":"2025-09-15T06:33:43.305791Z"}},"outputs":[{"name":"stdout","text":"Loading FEMNIST dataset...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd0e4d06a684474fbe3ef96e7f4ba2f6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"data/train-00000-of-00001.parquet:   0%|          | 0.00/201M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e05e6d6c9c9248fe95d5d98cf5631a30"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/814277 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c83849938d5049f980f23b4050c92272"}},"metadata":{}},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"ga = GeneticAlgorithm(population_size=6, mutation_rate=0.3, crossover_rate=0.7)\npopulation = ga.create_population()\n\ngenerations = 5\nbest_fitness_history = []\nbest_individual = None\nbest_fitness = 0.0","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:33:43.307205Z","iopub.execute_input":"2025-09-15T06:33:43.307712Z","iopub.status.idle":"2025-09-15T06:33:43.311711Z","shell.execute_reply.started":"2025-09-15T06:33:43.307680Z","shell.execute_reply":"2025-09-15T06:33:43.311083Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"print(f\"Starting GA optimization with {len(population)} individuals for {generations} generations...\")\n\nfor generation in range(generations):\n    print(f\"\\n---Generation {generation + 1}/{generations}---\")\n    fitness_scores = []\n    \n    for i, individual in enumerate(population):\n        print(f\"Evaluating individual {i+1}/{len(population)}...\")\n        \n        train_ds = FEMNISTDataset(ga_train_data, transform)\n        val_ds = FEMNISTDataset(ga_val_data, transform)\n        \n        train_loader = DataLoader(train_ds, batch_size=individual.batch_size, shuffle=True, num_workers=2)\n        val_loader = DataLoader(val_ds, batch_size=individual.batch_size, shuffle=False, num_workers=2)\n        \n        fitness = train_and_evaluate_model(individual, train_loader, val_loader, device)\n        fitness_scores.append(fitness)\n        \n        print(f\"->Fitness: {fitness:.4f}\")\n        print(f\"->Params: learning_rate={individual.learning_rate:.4f}, weight_decay={individual.weight_decay:.6f}, \"\n              f\"dropout_rate={individual.dropout_rate:.3f}, width_mult={individual.width_mult:.3f}\")\n        \n        if fitness > best_fitness:\n            best_fitness = fitness\n            best_individual = copy.deepcopy(individual)\n    \n    avg_fitness = np.mean(fitness_scores)\n    max_fitness = np.max(fitness_scores)\n    best_fitness_history.append(max_fitness)\n    \n    print(f\"Generation {generation + 1} - Avg: {avg_fitness:.4f}, Best: {max_fitness:.4f}\")\n    \n    if generation < generations - 1:\n        selected_parents = ga.select_parents(population, fitness_scores)\n        new_population = []\n        \n        best_idx = np.argmax(fitness_scores)\n        new_population.append(copy.deepcopy(population[best_idx]))\n        \n        while len(new_population) < ga.population_size:\n            parent1, parent2 = random.sample(selected_parents, 2)\n            if random.random() < ga.crossover_rate:\n                child1, child2 = ga.crossover(parent1, parent2)\n            else:\n                child1, child2 = copy.deepcopy(parent1), copy.deepcopy(parent2)\n            \n            child1 = ga.mutate(child1)\n            child2 = ga.mutate(child2)\n            \n            new_population.extend([child1, child2])\n        \n        population = new_population[:ga.population_size]\n\nprint(f\"\\n---GA Optimization Done---\")\nprint(f\"Best fitness: {best_fitness:.4f}\")\nprint(f\"Best hyperparams:\")\nprint(f\"->Learning rate: {best_individual.learning_rate:.6f}\")\nprint(f\"->Weight decay: {best_individual.weight_decay:.6f}\")\nprint(f\"->Dropout rate: {best_individual.dropout_rate:.4f}\")\nprint(f\"->Width multiplier: {best_individual.width_mult:.4f}\")\nprint(f\"->Batch size: {best_individual.batch_size}\")\nprint(f\"->Momentum: {best_individual.momentum:.4f}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:33:43.313036Z","iopub.execute_input":"2025-09-15T06:33:43.313631Z","iopub.status.idle":"2025-09-15T06:42:43.721058Z","shell.execute_reply.started":"2025-09-15T06:33:43.313613Z","shell.execute_reply":"2025-09-15T06:42:43.720197Z"}},"outputs":[{"name":"stdout","text":"Starting GA optimization with 6 individuals for 5 generations...\n\n---Generation 1/5---\nEvaluating individual 1/6...\n->Fitness: 0.3042\n->Params: learning_rate=0.0100, weight_decay=0.000200, dropout_rate=0.200, width_mult=1.000\nEvaluating individual 2/6...\n->Fitness: 0.1936\n->Params: learning_rate=0.0061, weight_decay=0.000706, dropout_rate=0.173, width_mult=1.139\nEvaluating individual 3/6...\n->Fitness: 0.3690\n->Params: learning_rate=0.0295, weight_decay=0.000159, dropout_rate=0.227, width_mult=1.241\nEvaluating individual 4/6...\n->Fitness: 0.2355\n->Params: learning_rate=0.0062, weight_decay=0.003510, dropout_rate=0.315, width_mult=1.005\nEvaluating individual 5/6...\n->Fitness: 0.3355\n->Params: learning_rate=0.0175, weight_decay=0.000807, dropout_rate=0.328, width_mult=0.949\nEvaluating individual 6/6...\n->Fitness: 0.3522\n->Params: learning_rate=0.0147, weight_decay=0.001906, dropout_rate=0.131, width_mult=0.778\nGeneration 1 - Avg: 0.2983, Best: 0.3690\n\n---Generation 2/5---\nEvaluating individual 1/6...\n->Fitness: 0.3616\n->Params: learning_rate=0.0295, weight_decay=0.000159, dropout_rate=0.227, width_mult=1.241\nEvaluating individual 2/6...\n->Fitness: 0.4010\n->Params: learning_rate=0.0295, weight_decay=0.000159, dropout_rate=0.135, width_mult=1.241\nEvaluating individual 3/6...\n->Fitness: 0.4316\n->Params: learning_rate=0.0147, weight_decay=0.001906, dropout_rate=0.054, width_mult=1.013\nEvaluating individual 4/6...\n->Fitness: 0.1794\n->Params: learning_rate=0.0081, weight_decay=0.010000, dropout_rate=0.149, width_mult=0.778\nEvaluating individual 5/6...\n->Fitness: 0.3658\n->Params: learning_rate=0.0295, weight_decay=0.000159, dropout_rate=0.227, width_mult=1.241\nEvaluating individual 6/6...\n->Fitness: 0.5576\n->Params: learning_rate=0.0449, weight_decay=0.000159, dropout_rate=0.227, width_mult=1.206\nGeneration 2 - Avg: 0.3828, Best: 0.5576\n\n---Generation 3/5---\nEvaluating individual 1/6...\n->Fitness: 0.5579\n->Params: learning_rate=0.0449, weight_decay=0.000159, dropout_rate=0.227, width_mult=1.206\nEvaluating individual 2/6...\n->Fitness: 0.4456\n->Params: learning_rate=0.0449, weight_decay=0.000159, dropout_rate=0.135, width_mult=1.049\nEvaluating individual 3/6...\n->Fitness: 0.4271\n->Params: learning_rate=0.0295, weight_decay=0.000159, dropout_rate=0.227, width_mult=0.832\nEvaluating individual 4/6...\n->Fitness: 0.5732\n->Params: learning_rate=0.0449, weight_decay=0.000159, dropout_rate=0.227, width_mult=1.206\nEvaluating individual 5/6...\n->Fitness: 0.4439\n->Params: learning_rate=0.0147, weight_decay=0.001906, dropout_rate=0.062, width_mult=1.013\nEvaluating individual 6/6...\n->Fitness: 0.2934\n->Params: learning_rate=0.0095, weight_decay=0.000159, dropout_rate=0.227, width_mult=1.206\nGeneration 3 - Avg: 0.4569, Best: 0.5732\n\n---Generation 4/5---\nEvaluating individual 1/6...\n->Fitness: 0.6072\n->Params: learning_rate=0.0488, weight_decay=0.001110, dropout_rate=0.227, width_mult=1.437\nEvaluating individual 3/6...\n->Fitness: 0.5677\n->Params: learning_rate=0.0449, weight_decay=0.000159, dropout_rate=0.234, width_mult=1.206\nEvaluating individual 4/6...\n->Fitness: 0.5821\n->Params: learning_rate=0.0449, weight_decay=0.000145, dropout_rate=0.227, width_mult=1.337\nEvaluating individual 5/6...\n->Fitness: 0.5760\n->Params: learning_rate=0.0449, weight_decay=0.000159, dropout_rate=0.149, width_mult=1.206\nEvaluating individual 6/6...\n->Fitness: 0.6566\n->Params: learning_rate=0.0449, weight_decay=0.000159, dropout_rate=0.143, width_mult=1.583\nGeneration 4 - Avg: 0.5900, Best: 0.6566\n\n---Generation 5/5---\nEvaluating individual 1/6...\n->Fitness: 0.6578\n->Params: learning_rate=0.0449, weight_decay=0.000159, dropout_rate=0.143, width_mult=1.583\nEvaluating individual 2/6...\n->Fitness: 0.6706\n->Params: learning_rate=0.0449, weight_decay=0.000159, dropout_rate=0.003, width_mult=1.052\nEvaluating individual 3/6...\n->Fitness: 0.6559\n->Params: learning_rate=0.0449, weight_decay=0.000159, dropout_rate=0.143, width_mult=1.583\nEvaluating individual 4/6...\n->Fitness: 0.6403\n->Params: learning_rate=0.0488, weight_decay=0.001110, dropout_rate=0.143, width_mult=1.437\nEvaluating individual 5/6...\n->Fitness: 0.6026\n->Params: learning_rate=0.0449, weight_decay=0.000159, dropout_rate=0.227, width_mult=1.583\nEvaluating individual 6/6...\n->Fitness: 0.6771\n->Params: learning_rate=0.0775, weight_decay=0.000159, dropout_rate=0.143, width_mult=1.857\nGeneration 5 - Avg: 0.6507, Best: 0.6771\n\n---GA Optimization Done---\nBest fitness: 0.6771\nBest hyperparams:\n->Learning rate: 0.077489\n->Weight decay: 0.000159\n->Dropout rate: 0.1426\n->Width multiplier: 1.8574\n->Batch size: 64\n->Momentum: 0.8017\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"full_train_ds = FEMNISTDataset(train_valid['train'], transform)\nfull_val_ds = FEMNISTDataset(train_valid['test'], transform)\nfull_test_ds = FEMNISTDataset(test_split, transform)\n\ntrain_loader = DataLoader(full_train_ds, batch_size=best_individual.batch_size, shuffle=True, num_workers=2)\nval_loader = DataLoader(full_val_ds, batch_size=best_individual.batch_size, shuffle=False, num_workers=2)\ntest_loader = DataLoader(full_test_ds, batch_size=best_individual.batch_size, shuffle=False, num_workers=2)\n\nfinal_model = CNN(num_classes=62, hyperparams=best_individual).to(device)\noptimizer = torch.optim.SGD(\n    final_model.parameters(),\n    lr=best_individual.learning_rate,\n    momentum=best_individual.momentum,\n    weight_decay=best_individual.weight_decay\n)\ncriterion = nn.CrossEntropyLoss()\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=10)\n\nprint(f\"Final model parameters: {sum(p.numel() for p in final_model.parameters())}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:43:37.471008Z","iopub.execute_input":"2025-09-15T06:43:37.471583Z","iopub.status.idle":"2025-09-15T06:43:37.482558Z","shell.execute_reply.started":"2025-09-15T06:43:37.471555Z","shell.execute_reply":"2025-09-15T06:43:37.481936Z"}},"outputs":[{"name":"stdout","text":"Final model parameters: 54396\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"print(f\"\\nTraining Model with optimised hyperparams...\")\n\nepochs = 10\nbest_val_acc = 0.0\n\nfor epoch in range(epochs):\n    final_model.train()\n    train_loss = 0\n    train_correct = 0\n    train_total = 0\n    \n    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n    for batch_idx, (data, target) in enumerate(pbar):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = final_model(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        \n        train_loss += loss.item()\n        pred = output.argmax(dim=1)\n        train_correct += pred.eq(target).sum().item()\n        train_total += target.size(0)\n        \n        if batch_idx % 500 == 0:\n            pbar.set_postfix({\n                'loss': f'{train_loss/(batch_idx+1):.4f}',\n                'acc': f'{100.*train_correct/train_total:.2f}%'\n            })\n    \n    final_model.eval()\n    val_correct = 0\n    val_total = 0\n    val_loss = 0\n    \n    with torch.no_grad():\n        for data, target in val_loader:\n            data, target = data.to(device), target.to(device)\n            output = final_model(data)\n            loss = criterion(output, target)\n            val_loss += loss.item()\n            pred = output.argmax(dim=1)\n            val_correct += pred.eq(target).sum().item()\n            val_total += target.size(0)\n    \n    val_acc = val_correct / val_total\n    train_acc = train_correct / train_total\n    \n    print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n    \n    if val_acc > best_val_acc:\n        best_val_acc = val_acc\n        torch.save(final_model.state_dict(), \"ga_optimized_model.pth\")\n    \n    scheduler.step()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T06:43:42.384065Z","iopub.execute_input":"2025-09-15T06:43:42.384320Z","iopub.status.idle":"2025-09-15T07:19:32.739244Z","shell.execute_reply.started":"2025-09-15T06:43:42.384302Z","shell.execute_reply":"2025-09-15T07:19:32.738233Z"}},"outputs":[{"name":"stdout","text":"\nTraining Model with optimised hyperparams...\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/10: 100%|██████████| 10306/10306 [03:15<00:00, 52.81it/s, loss=0.8422, acc=74.83%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Acc: 0.7497, Val Acc: 0.8338\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/10: 100%|██████████| 10306/10306 [03:13<00:00, 53.23it/s, loss=0.6214, acc=80.20%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Acc: 0.8021, Val Acc: 0.8391\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/10: 100%|██████████| 10306/10306 [03:13<00:00, 53.16it/s, loss=0.5924, acc=81.00%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Acc: 0.8101, Val Acc: 0.8461\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/10: 100%|██████████| 10306/10306 [03:14<00:00, 52.89it/s, loss=0.5746, acc=81.54%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Acc: 0.8153, Val Acc: 0.8424\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/10: 100%|██████████| 10306/10306 [03:15<00:00, 52.67it/s, loss=0.5574, acc=82.05%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Acc: 0.8204, Val Acc: 0.8483\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/10: 100%|██████████| 10306/10306 [03:15<00:00, 52.60it/s, loss=0.5431, acc=82.51%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Acc: 0.8250, Val Acc: 0.8516\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/10: 100%|██████████| 10306/10306 [03:15<00:00, 52.73it/s, loss=0.5277, acc=82.92%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Acc: 0.8290, Val Acc: 0.8559\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/10: 100%|██████████| 10306/10306 [03:13<00:00, 53.30it/s, loss=0.5126, acc=83.38%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Acc: 0.8338, Val Acc: 0.8599\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/10: 100%|██████████| 10306/10306 [03:13<00:00, 53.35it/s, loss=0.4955, acc=83.87%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Acc: 0.8387, Val Acc: 0.8612\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/10: 100%|██████████| 10306/10306 [03:11<00:00, 53.77it/s, loss=0.4850, acc=84.13%]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Acc: 0.8414, Val Acc: 0.8637\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"final_model.load_state_dict(torch.load(\"ga_optimized_model.pth\", map_location=device))\nfinal_model.eval()\n\ntest_correct = 0\ntest_total = 0\n\nwith torch.no_grad():\n    for data, target in tqdm(test_loader, desc=\"Final Test\"):\n        data, target = data.to(device), target.to(device)\n        output = final_model(data)\n        pred = output.argmax(dim=1)\n        test_correct += pred.eq(target).sum().item()\n        test_total += target.size(0)\n\nfinal_test_acc = test_correct / test_total\n\nprint(f\"\\nResults:\")\nprint(f\"Final Test Accuracy: {final_test_acc:.4f}\")\nprint(f\"Model saved as 'ga_optimized_model.pth'\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T07:19:40.151213Z","iopub.execute_input":"2025-09-15T07:19:40.151560Z","iopub.status.idle":"2025-09-15T07:20:03.327684Z","shell.execute_reply.started":"2025-09-15T07:19:40.151530Z","shell.execute_reply":"2025-09-15T07:20:03.326776Z"}},"outputs":[{"name":"stderr","text":"Final Test: 100%|██████████| 1273/1273 [00:23<00:00, 54.97it/s]","output_type":"stream"},{"name":"stdout","text":"\nResults:\nFinal Test Accuracy: 0.8638\nModel saved as 'ga_optimized_model.pth'\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":16}]}